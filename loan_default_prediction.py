# -*- coding: utf-8 -*-
"""Loan Default Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gAE4LdP5n6pQhY-C7OYmS0i8LXAfQBMz

# ***INSTALL OUTSIDE LIBRARIES***
"""

!pip install lazypredict

!pip install optuna

!pip install featuretools

"""# ***IMPORT DEPENDENCIES***"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from lazypredict.Supervised import LazyClassifier

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import optuna

"""# ***IMPORTING DATASET***"""

df = pd.read_csv("/content/loan_dataset.csv")
df.head()

df = df.drop("Loan_ID",axis=1)

df.head()

"""# ***DATA EDA***"""

df.shape

df.info()

df.describe()

df.isnull().sum()

"""# ***MISSING VALUES***"""

# FILLING THE MISSING VALUES
from sklearn.impute import SimpleImputer

cat_cols = ["Gender", "Married", "Dependents", "Self_Employed", "Credit_History"]
cat_imputer = SimpleImputer(strategy="most_frequent")
df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])

num_cols = ["ApplicantIncome", "CoapplicantIncome", "LoanAmount"]
num_imputer = SimpleImputer(strategy="median")
df[num_cols] = num_imputer.fit_transform(df[num_cols])

df["Loan_Amount_Term"].fillna(df["Loan_Amount_Term"].mode()[0], inplace=True)

df.isnull().sum()

"""# ***VISULIZATION AND TRANSFORMATION***"""

cat_cols = ["Gender", "Married", "Dependents", "Education", "Self_Employed", "Credit_History"]

plt.figure(figsize=(10, 8))
for i, col in enumerate(cat_cols, 1):
    plt.subplot(2, 3, i)
    sns.countplot(data=df, x=col, palette="Set2")
    plt.title(f"Countplot of {col}")
    plt.xticks(rotation=30)

plt.tight_layout()
plt.show()

num_cols = ["ApplicantIncome", "CoapplicantIncome", "LoanAmount", "Loan_Amount_Term"]


plt.figure(figsize=(10, 8))
for i, col in enumerate(num_cols, 1):
    plt.subplot(2, 2, i)
    sns.histplot(df[col].dropna(), bins=30, kde=True, color="skyblue", edgecolor="black")
    plt.title(f"Distribution of {col}")

plt.tight_layout()
plt.show()

from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer(method="yeo-johnson")
df[["ApplicantIncome", "LoanAmount","CoapplicantIncome", "Loan_Amount_Term"]] = pt.fit_transform(df[["ApplicantIncome", "LoanAmount","CoapplicantIncome", "Loan_Amount_Term"]])

num_cols = ["ApplicantIncome", "LoanAmount","CoapplicantIncome", "Loan_Amount_Term"]


plt.figure(figsize=(10, 8))
for i, col in enumerate(num_cols, 1):
    plt.subplot(2, 2, i)
    sns.histplot(df[col].dropna(), bins=30, kde=True, color="skyblue", edgecolor="black")
    plt.title(f"Distribution of {col}")

plt.tight_layout()
plt.show()

df.head()

df.head()

"""# ***ENCODING***"""

from sklearn.preprocessing import LabelEncoder

# Columns you shared
cat_cols = ["Gender", "Married", "Dependents", "Education",
            "Self_Employed", "Property_Area", "Loan_Status"]

# Make a copy to avoid overwriting raw data
encoded_df = df.copy()

# Label Encoding for binary or ordinal columns
label_enc_cols = ["Gender", "Married", "Education", "Self_Employed", "Loan_Status"]

le = LabelEncoder()
for col in label_enc_cols:
    encoded_df[col] = le.fit_transform(encoded_df[col])

# One-Hot Encoding for nominal categorical columns
one_hot_cols = ["Dependents", "Property_Area"]

encoded_df = pd.get_dummies(encoded_df, columns=one_hot_cols)

df_final = pd.DataFrame(encoded_df)
df_final.head()

# convert all boolean columns to integers (0/1)
df_final = df_final.astype({col: int for col in encoded_df.select_dtypes(bool).columns})
df_final = df_final.astype({col: int for col in encoded_df.select_dtypes(object).columns})
df_final.head()

df_final_model.shape

"""# ***DATA BALANCING***"""

len(df_final[df_final["Loan_Status"] == 1])

len(df_final[df_final["Loan_Status"] == 0])

from imblearn.over_sampling import SMOTE

X = df_final.drop("Loan_Status", axis=1)
y = df_final["Loan_Status"]

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print(pd.Series(y_resampled).value_counts())

df_final_model = pd.concat([X_resampled, y_resampled], axis=1)
df_final_model.head()

df_final_model.shape

"""# ***CHECKING BEST MODEL ON THIS DATASET***"""

X = df_final_model.drop("Loan_Status", axis=1)
y = df_final_model["Loan_Status"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train

X_train.shape

X_test.shape

from lazypredict.Supervised import LazyClassifier
# LazyClassifier
clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)
models, predictions = clf.fit(X_train, X_test, y_train, y_test)
print(models)

X_train.head(2)

"""# ***MODEL TRAINING***

# ***MODEL 1***
"""

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix

# BASE MODEL

model_1 = XGBClassifier(n_estimators=100, random_state=42)
model_1.fit(X_train, y_train)
y_pred = model_1.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

from sklearn.model_selection import cross_val_score , KFold
from sklearn.metrics import make_scorer, f1_score, roc_auc_score

scoring = {
    'accuracy': 'accuracy',
    'balanced_accuracy': 'balanced_accuracy',
    'f1': make_scorer(f1_score),
    'roc_auc': 'roc_auc'
}

scores = {}
for metric, scorer in scoring.items():
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(model_1, X, y, cv=cv, scoring=scorer)
    scores[metric] = (cv_scores.mean(), cv_scores.std())

score = pd.DataFrame(scores, index=['Mean', 'Std Dev'])
print(score)

classification_report = classification_report(y_test, y_pred)
print(classification_report)

confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)

# HYPERPARAMETER TUNING OF MODEL_1
def objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 100, 1000),
        "max_depth": trial.suggest_int("max_depth", 3, 15),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
        "gamma": trial.suggest_float("gamma", 0, 5),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
        "random_state": 42,
        "use_label_encoder": False,
        "eval_metric": "logloss"
    }

    model = XGBClassifier(**params)

    # Stratified K-Fold CV
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X, y, cv=cv, scoring="accuracy")

    return scores.mean()

# Run optuna
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=500)

print("Best Hyperparameters:", study.best_params)
print("Best Accuracy:", study.best_value)

# IMPROVE MODEL

Hyperparameters = {'n_estimators': 269,
                   'max_depth': 10,
                   'learning_rate': 0.024531319912071248,
                   'subsample': 0.9147699225691904,
                   'colsample_bytree': 0.9486632907001317,
                   'gamma': 0.3807799542798858,
                   'min_child_weight': 2
}


model_xgb = XGBClassifier(**Hyperparameters, random_state=42)
model_xgb.fit(X_train, y_train)

scoring = {
    'accuracy': 'accuracy',
    'balanced_accuracy': 'balanced_accuracy',
    'f1': make_scorer(f1_score),
    'roc_auc': 'roc_auc'
}

scores = {}
for metric, scorer in scoring.items():
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(model_xgb, X, y, cv=cv, scoring=scorer)
    scores[metric] = (cv_scores.mean(), cv_scores.std())

score = pd.DataFrame(scores, index=['Mean', 'Std Dev'])
print(score)

y_test

joblib.dump(model_xgb, "final_model.pkl")

"""# ***MODEL 2***"""

from sklearn.ensemble import ExtraTreesClassifier

# BASE MODEL
model_2 = ExtraTreesClassifier(random_state=42)
model_2.fit(X_train, y_train)
y_pred = model_2.predict(X_test)

scoring = {
    'accuracy': 'accuracy',
    'balanced_accuracy': 'balanced_accuracy',
    'f1': make_scorer(f1_score),
    'roc_auc': 'roc_auc'
}

scores = {}
for metric, scorer in scoring.items():
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(model_2, X, y, cv=cv, scoring=scorer)
    scores[metric] = (cv_scores.mean(), cv_scores.std())

score = pd.DataFrame(scores, index=['Mean', 'Std Dev'])
print(score)

# HYPERPARAMETER TUNING OF ExtraTreesClassifier
from sklearn.ensemble import ExtraTreesClassifier

def objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 100, 1000),
        "max_depth": trial.suggest_int("max_depth", 3, 50),
        "min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
        "min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 20),
        "max_features": trial.suggest_categorical("max_features", ["sqrt", "log2", None]),
        "bootstrap": trial.suggest_categorical("bootstrap", [True, False]),
        "random_state": 42
    }

    model = ExtraTreesClassifier(**params)

    # Stratified K-Fold CV
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X, y, cv=cv, scoring="accuracy")

    return scores.mean()

# Run optuna
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=200)

print("Best Hyperparameters:", study.best_params)
print("Best Accuracy:", study.best_value)

# IMPROVE MODEL

Hyperparameters = {'n_estimators': 366,
                   'max_depth': 50,
                   'min_samples_split': 3,
                   'min_samples_leaf': 1,
                   'max_features': 'sqrt',
                   'bootstrap': False
}

model_etc = ExtraTreesClassifier(**Hyperparameters,random_state=42)
model_etc.fit(X_train, y_train)
y_pred = model_etc.predict(X_test)

scoring = {
    'accuracy': 'accuracy',
    'balanced_accuracy': 'balanced_accuracy',
    'f1': make_scorer(f1_score),
    'roc_auc': 'roc_auc'
}

scores = {}
for metric, scorer in scoring.items():
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(model_etc, X, y, cv=cv, scoring=scorer)
    scores[metric] = (cv_scores.mean(), cv_scores.std())

score = pd.DataFrame(scores, index=['Mean', 'Std Dev'])
print(score)

"""# ***MODEL 3***"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# BASE MODEL
model_3 = LinearDiscriminantAnalysis()
model_3.fit(X_train, y_train)
y_pred = model_3.predict(X_test)

scoring = {
    'accuracy': 'accuracy',
    'balanced_accuracy': 'balanced_accuracy',
    'f1': make_scorer(f1_score),
    'roc_auc': 'roc_auc'
}

scores = {}
for metric, scorer in scoring.items():
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(model_3, X, y, cv=cv, scoring=scorer)
    scores[metric] = (cv_scores.mean(), cv_scores.std())

score = pd.DataFrame(scores, index=['Mean', 'Std Dev'])
print(score)

def objective(trial):
    solver = trial.suggest_categorical("solver", ["svd", "lsqr", "eigen"])

    # shrinkage only works with 'lsqr' or 'eigen'
    shrinkage = None
    if solver in ["lsqr", "eigen"]:
        shrinkage = trial.suggest_categorical("shrinkage", ["auto", None])  # You can also try floats

    lda = LinearDiscriminantAnalysis(
        solver=solver,
        shrinkage=shrinkage
    )

    # Cross-validation
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(lda, X, y, cv=cv, scoring="accuracy")

    return np.mean(scores)

# Run optimization
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=200)

print("Best trial:")
trial = study.best_trial
print("  Accuracy: {}".format(trial.value))
print("  Best params: {}".format(trial.params))

# IMPROVE MODEL

params = {'solver': 'svd'}

model_lda = LinearDiscriminantAnalysis()
model_lda.fit(X_train, y_train)
y_pred = model_lda.predict(X_test)

scoring = {
    'accuracy': 'accuracy',
    'balanced_accuracy': 'balanced_accuracy',
    'f1': make_scorer(f1_score),
    'roc_auc': 'roc_auc'
}

scores = {}
for metric, scorer in scoring.items():
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(model_lda, X, y, cv=cv, scoring=scorer)
    scores[metric] = (cv_scores.mean(), cv_scores.std())

score = pd.DataFrame(scores, index=['Mean', 'Std Dev'])
print(score)

"""# ***PREDICTION***"""

# Example: take the first 5 rows of X_test as raw input
raw_input_demo = X_test.head(5)
print("Raw input data:")
print(raw_input_demo)

# Predict using the reusable pipeline
y_pred_1 = model_xgb.predict(raw_input_demo)

print("\nPredicted target values:")
print(y_pred_1)

y_test.head(5)

"""# ***MAKING PIPELINE***"""

df_final_model.head(2)

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
import joblib


# Columns
label_cols = ["Gender", "Married", "Education", "Self_Employed", "Credit_History"]
num_cols = ["ApplicantIncome", "LoanAmount", "CoapplicantIncome", "Loan_Amount_Term"]
onehot_cols = ["Dependents", "Property_Area"]

# Pipelines
label_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OrdinalEncoder())
])

num_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("power", PowerTransformer(method="yeo-johnson"))
])

onehot_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

# Combine
preprocessor = ColumnTransformer([
    ("label_cat", label_pipeline, label_cols),
    ("num", num_pipeline, num_cols),
    ("onehot_cat", onehot_pipeline, onehot_cols)
])

# Fit on training data
X_train = df.drop("Loan_Status", axis=1)
preprocessor.fit(X_train)

# Save pipeline
joblib.dump(preprocessor, "preprocessor_pipeline.pkl")

import joblib
from sklearn.pipeline import Pipeline
import pandas as pd

# Load fitted preprocessing pipeline
preprocessor = joblib.load("preprocessor_pipeline.pkl")

# Load trained model (XGBClassifier)
model = joblib.load("final_model.pkl")

import joblib
import pandas as pd

# Load preprocessor and model
preprocessor = joblib.load("preprocessor_pipeline.pkl")
model = joblib.load("final_model.pkl")

# New raw input
new_data = pd.DataFrame([{
    "Gender": "Male",
    "Married": "Yes",
    "Dependents": "0",
    "Education": "Graduate",
    "Self_Employed": "Yes",
    "ApplicantIncome": 3000,
    "CoapplicantIncome": 0.00,
    "LoanAmount": 66.00,
    "Loan_Amount_Term": 360,
    "Credit_History": 1.0,
    "Property_Area": "Urban"
}])

# Transform the data
X_new = preprocessor.transform(new_data)

# Predict
y_pred = model.predict(X_new)
print("Prediction:", y_pred)

